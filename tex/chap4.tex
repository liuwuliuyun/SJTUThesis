\chapter{人脸匹配算法}

\section{概论}

由特征提取算法得到人脸特征向量之后，接下来就需要将这个特征向量与人脸标准数据库中已有的特征向量进行匹配，找到距离其最近的特征向量，从而确定待检测人脸的身份。如果数据库中的数据量较少，线性搜索的性能就足以满足系统需求。但在数据量较大，例如数据库中有百万以上的特征向量时，线性搜索需要较多的时间，无法满足系统实时性的需求。因此，我们需要更高效的搜索算法加速这一过程。

我们对合格的人脸匹配算法有四点要求：

第一是准确，一次查询的结果应当非常接近线性搜索的结果。第二是快速，算法的时间复杂度为不超过$O(n)$。第三是高效，即理想情况下，数据的索引应当和数据集成线性关系。第四是高维，由于人脸特征向量的维度常在100维以上，算法需要支持对高维特征的高效搜索。

除了线性搜索，常见的精确搜索算法有基于树形结构的R-tree和K-D tree等算法，根据一项研究结论，在维度超过10的时候，这些精确算法的时间效率已经超过了线性搜索\cite{weber1998quantitative}，而人脸特征向量的维度基本上在100的级别。所以使用这些精确搜索算法无法满足高维的要求，这些算法都不合适用作本系统的匹配算法。

而在机器学习的领域，支持向量机(SVM)常用于高维向量的分类和回归。而在人脸向量的匹配阶段，如果人脸特征数据库内存在$n$个特征向量， 那么我们使用SVM将找到查询向量$\mathbf{q}$所匹配的特征向量共有两种思路：

思路一，训练$n$个分类器，每个分类器都将人脸特征库中的某一个特征向量与剩余的所有特征向量区分开。在使用时，需要每个分类器对查询向量$\mathbf{q}$进行分类，如果$\mathbf{q}$与某个特征向量分到了同一类，则将该特征向量作为$\mathbf{q}$的匹配候选项。

思路二，训练$C_n^2$个分类器，将$n$个特征向量中任意挑出的一对不同的特征向量区分开。需要找到查询向量$\mathbf{q}$的匹配项时，使用每个训练好的分类器对$\mathbf{q}$进行分类，每次将分类器输出结果所表示的特征向量类别权重加一。所有的分类器分类完毕时，选择权重最高的一类中的特征向量即为匹配结果。

这两种思路在$n$不大的时候都是可行的。事实上，在一些简单的人脸识别系统中，使用多类支持向量机(Multi-Class SVM)匹配高维人脸特征向量的做法非常普遍。

然而对于思路一中的方法，当$n$非常大的时候，需要训练的分类器的数量为$O(n)$，而且每次查询的时间复杂度也为$O(n)$；对于思路二中的方法，当$n$非常大的时候，需要训练的分类器的数量为$O(n^2)$，每次查询的时间复杂度也为$O(n^2)$。在简单分析后我们可以看出，多类支持向量机不符合快速的要求，当人脸特征向量的数量非常大的时候，不适合作为人脸匹配算法使用。

我们注意到人脸特征向量的具有这样的特性：相同身份的人脸会产生距离较近的特征向量，而不同身份的人脸往往会产生距离较远的特征向量。由此可将人脸特征向量的匹配过程等同于在人脸特征向量数据库中寻找距离查询向量$\mathbf{q}$距离最近的一项。通过这样的转换，我们将匹配问题等价于最近邻搜索问题。

最近邻搜索问题的解决主要有两大类方法，精确搜索和近似搜索。其中近似搜索是一类能够以非常高的概率找到最近邻的搜索方法。

\section{基于局部敏感哈希的近似搜索}

\subsection{LSH函数族的定义与生成}

局部敏感哈希(LSH)在最近邻的搜索问题上有着广泛的应用，是一种近似搜索方法。

LSH算法的核心是LSH函数族，以下为LSH函数族的定义：

一个LSH函数族$\mathcal{F}$定义在矩阵空间$\mathcal{M}=(M,d)$上，其中阈值$R>0$，渐进系数$c>1$。LSH函数族中的每一个函数$h:\mathcal{M}\rightarrow S$都将元素从矩阵空间映射到桶$s\in S$中，并满足使用随机平均抽取的一个函数$h\in \mathcal{F}$，为对任意的两个点$p,q\in \mathcal{M}$，都有如下特性：

\begin{itemize}
	\item 若$d(p,q)\leq R$，那么$h(p)=h(q)$的概率至少为$P_1$。
	\item 若$d(p,q)\geq cR$，那么$h(p)=h(q)$的概率至多为$P_2$。
\end{itemize}

在最邻近搜索中使用的LSH函数族均满足$P_1>P_2$的条件，此时，这个LSH函数族被称为$(R,cR,P_1,P_2)-sensitive$。

如果给定一个$(d_1,d_2,p_1,p_2)-sensitive$的LSH函数族$\mathcal{F}$，我们可以通过与或的操作创建新的LSH函数族。

采用求与的方法，定义一个新的函数族$\mathcal{G}$，函数族中每一个哈希函数为$g$。每一个$g$均由函数族$\mathcal{F}$中的$k$个随机选取的哈希函数$h_1,...,h_k$构成。对于哈希函数$g\in \mathcal{G}$，$g(x)=g(y)$当且仅当$i=1,2,...,k$时，所有的$h_i(x)=h_i(y)$。此时生成$\mathcal{G}$为$(d_1,d_2,p_1^k,p_2^k)-sensitive$的LSH函数族。

采用求或的方法，定义一个新的函数族$\mathcal{G}$，函数族中每一个哈希函数为$g$。每一个$g$均由函数族$\mathcal{F}$中的$k$个随机选取的哈希函数$h_1,...,h_k$构成。对于哈希函数$g\in \mathcal{G}$，$g(x)=g(y)$当且仅当存在一个或者多个$i$，使得$h_i(x)=h_i(y)$。此时生成$\mathcal{G}$为$(d_1,d_2,1-(1-p_1^k),1-(1-p_2^k))-sensitive$的LSH函数族。

\subsection{基础LSH最邻近搜索算法}

利用LSH函数族进行最近邻搜索的算法有两个主要参数，宽度$k$和哈希表的数量$L$，其中宽度$k$由创建过程中随机选取哈希函数的数量$k$定义。

第一步，采用上述的求与或求或的方法创建新的LSH函数族$\mathcal{G}$，接着由LSH函数族$\mathcal{G}$中随机选取$L$个不同的哈希函数，创建$L$个哈希表。

第二步，对于查询向量$\mathbf{q}$，遍历每一个哈希表，取出与其分配到同一桶中的的向量将其放到集合$S$中。

第三步，使用线性搜索的方法找到$S$中最邻近$\mathbf{q}$的向量。

\subsection{基础LSH最邻近搜索算法复杂度分析}
令
\begin{equation}
	k=\frac{log n}{log 1/P_2},L=n^\rho
\end{equation}

其中
\begin{equation}
	\rho =\frac{log P_1}{log P_2}
\end{equation}

则该算法的空间复杂度为$O(n^{1+\rho})$，时间复杂度为$O(n^\rho)$。由于$0<\rho <1$，所以该算法的搜索的时间效率高于线性搜索。

从工程实践的角度出发，基础LSH算法的最大问题是需要非常多的哈希表，才能在保证准确性的同时找到绝大多数的邻近元素。而每个哈希表的空间大小是和数据集的大小成比例的，所以数据量越大，需要的存储空间就越大。如果哈希表的存储空间超过了内存的容量，那么每一次查询操作就会涉及到磁盘的读取，从而极大的增加每次查询操作的时间开销。

\subsection{基础LSH最邻近算法的优化思路}

优化基础LSH最邻近算法的一种思路是减少查询时所需要的哈希表的数量。假设我们知道待求的最邻近向量$\mathbf{p}$和查询向量$\mathbf{q}$之间的距离$R_p$，那么原则上，我们可以计算出$\mathbf{p}$与$\mathbf{q}$分在同一个桶中的概率。如果定义这个概率叫做成功率，那么查询的时候只需要查找成功率高的桶就可以避免使用过多无用的哈希表，从而提高存储的利用率。

然而预先计算每个桶的成功率在实际情况下是十分复杂的，所以，我们需要一种方法来高效的进行这种计算。每次查询之前，随机生成一个与查询向量$\mathbf{q}$相距$R_p$的点$\mathbf{p'}$。标记$\mathbf{p'}$经过哈希后落到的桶。这样保证了所有的桶取样概率的正确性。多次执行同样的操作，即可找出所有高成功率的桶。

这样的思路以牺牲时间效率为代价换取了更高的空间利用率，避免了内存不够而需要访问硬盘的情况。这样的优化方式使得需要的哈希表数量减少了$1/2$到$2/3$\cite{lv2007multi}。不过由于每次查询前都增加了取样的过程，使得每次查询的时间增加了$30\%$到$210\%$不等。

除了每次查询需要更多的时间之外，在实际测试中还发现了上述思路存在其他一些缺陷。

第一，在一次查询中高成功率的桶会被不同的扰动点$\mathbf{p'}$产生多次，即在一次查询中做了多次重复计算。通过记住每个扰动点生成的桶可以避免此问题，而其需要的空间开销在高并发量的查询时仍然是不可接受的。

第二，对于每一个待检测的数据集，参数$R_p$需要人工调整。若$R_p$偏大，则取样结果过多，容易产生误判；若$R_p$偏小，则取样结果过少，容易产生缺漏。

针对上述的缺陷，Qin Lv等人提出了多探针LSH\cite{lv2007multi}，以在减少哈希表数量的同时可以高效的执行查询操作。

设$\mathbf{a}$为从正态分布中随机取出的$d$维向量，$b$是从$[0,W]$均匀分布中随机取出的一个实数，则LSH函数族中的每一个哈希函数被定义为：

\begin{equation}
	h_{\mathbf{a},b}(\mathbf{v})=\Big\lfloor\frac{\mathbf{a}\cdot \mathbf{v}+b}{W} \Big\rfloor
\end{equation}

根据这个哈希函数的性质，如果$\mathbf{p}$，$\mathbf{q}$两点为邻近点，而经过哈希函数的计算后$\mathbf{p}$没有和$\mathbf{q}$分在同一个桶中，那么$\mathbf{p}$一定与$\mathbf{q}$分到了靠近的桶中。事实上，如果参数$W$选取合适，$\mathbf{p}$会有极大的概率分到与$\mathbf{q}$紧邻的桶中。

根据邻近点在桶中的分布特性，使用合适的方法推导出探针序列，即可使用探针将绝大多数查询向量$\mathbf{q}$的邻近点找出，从而得到最邻近点。

多探针LSH算法的测试采用了两个数据集，一个是包含130万图片的数据集，另一个是含有260万音频的数据集。测试结果表明，多探针LSH算法的空间利用率是基础LSH算法的14至18倍，而查询时间基本接近\cite{lv2007multi}。


\section{基于邻近图的近似搜索}

\subsection{概述}

基于LSH进行近似搜索的主要思想是相似的向量经过哈希函数的计算后仍然在相邻或者相同的桶里。这类算法在数据集中的高维向量距离相距较远的情况下存在明显的缺陷，即相邻的样本在计算会分布在稀疏的、不相邻的桶中，从而降低搜索效率。相比于LSH，数据集中的每个特征向量都由有向图中的一个顶点表示，构建有向图来描述数据集则可以很好的解决这个问题。Ben Harwood与Tom Drummond提出了一个高效的利用邻近图执行近似搜索的算法FANNG\cite{harwood2016fanng}。

\subsection{理想图的构建}

如何构建邻近图是这类算法的核心。而最优的邻近图是在理想图的基础上优化而来。因此，在得到邻近图之前首先需要构建一个理想图。当查询点$Q$为数据集中的一个点时，无论初始查询下标从哪里开始，算法\ref{algo:fanng1}总能正确的找到正确的匹配结果。符合这样要求的最小图定义为理想图。

\begin{algorithm}
	% \begin{algorithm}[H] % 强制定位
	\caption{向下搜索算法\cite{harwood2016fanng}}
	\label{algo:fanng1}
	\begin{algorithmic}[1] %每行显示行号
		\Require $P$理想图的顶点集，$E$理想图的边集，$Q$查询点，$v$初始查询下标 % 输入
		\Ensure $v$最邻近下标 % 输出
		\For {each edge $E_i$ with start vertex $P_v$}
		\State $u\leftarrow$ index of end vertex of $E_i$
		\If {$distance(Q,P_u)<distance(Q,P_v)$}
		\State $v\leftarrow u$
		\EndIf
		\EndFor
	\end{algorithmic}
\end{algorithm}

在这样的搜索过程中，每次迭代后当前的点与查询点之间的距离都会减小。因此，整个算法在有限次迭代之后一定会终止。这也同时意味着如果图中存在一条从$p_1$到$p_2$的边，那么任何一条从$p_1$出发到$p_3$的边，当$p_1$到$p_3$的距离比$p_2$到$p_3$的距离远时都没有存在的意义。

在理想图中，给定数据点$p_i$与距离函数$d:\mathbf{R}\times \mathbf{R} \rightarrow \mathbf{R}$，定义边$(p_1,p_2)$阻塞边$(p_1,p_3)$当且仅当$d(p_1,p_2)<d(p_1,p_3)$并且$d(p_2,p_3)<d(p_1,p_3)$。

明确了如何精简图之后，使用算法\ref{algo:fanng2}构建理想图。

\begin{algorithm}
	% \begin{algorithm}[H] % 强制定位
	\caption{理想图构建算法\cite{harwood2016fanng}}
	\label{algo:fanng2}
	\begin{algorithmic}[1] %每行显示行号
		\Require $P$理想图的顶点集
		\Ensure $E$理想图的边集 % 输出
		\For {each vertex $P_i$}
		\State $e\leftarrow$ empty sorted list of edges
		\For {each vertex $P_j\neq P_i$}
		\State $u \leftarrow$ index of end vertex of $e_j$
		\State $L \leftarrow$ length of $e_j$
		\State $occluded \leftarrow \mathbf{false}$
		\For {each edge $E_k$ with start vertex $P_i$}
		\State $v \leftarrow$ index of end vertex of $E_k$
		\If {$d(P_u,P_v)<L$}
		\State $occluded \leftarrow \mathbf{true}$
		\EndIf
		\EndFor
		\EndFor
		\EndFor
		\Return $E$
	\end{algorithmic}
\end{algorithm}

建立理想图之后我们需要探究其存储空间是否合理。尽管数据集中的数据可能是高维的，例如SIFT\cite{asuncion2007uci}数据集中的数据有128维，但是其固有维度却只有11维\cite{harwood2016fanng}。这使得使用SIFT数据集构建的理想图中每个点的度平均值为25左右。因此，图的存储空间是可以接受的。

不过此时我们要求查询点$Q$必须在数据集中，这样的要求限制了算法的运用场景。所以，我们将阻塞的定义修改为：边$(p_1,p_2)$阻塞边$(p_1,p_3)$当且仅当$d(p_1,p_2)<d(p_1,p_3)$并且$d(p_2,p_3)^2<d(p_1,p_3)^2-2\tau d(p_1,p_2)$。此时再使用算法\ref{algo:fanng2}即可构建寻找与查询点$Q$距离小于$\tau$的最邻近点的邻近图。

需要注意的是，如果使用了新的阻塞定义，会增加理想图中每个点的度，使得整个图的存储代价变大，同时也会使一次搜索的时间效率下降。因此，在实际使用时，需要根据需要权衡$\tau$的取值。如果$\tau$值选的过小，则很可能降低可能的最邻近点的候选数量；如果$\tau$值选的过大，则很可能增大存储图所需要的空间并降低搜索的时间效率。

\subsection{使用回溯进行快速搜索}

一个提高搜索效率的方法通过修改算法\ref{algo:fanng1}。新的算法\ref{algo:fanng3}不再像算法\ref{algo:fanng1}当无法继续时终止，而是使用了深度优先的回溯搜索从第二靠近的点出发，检查依然没有被搜索过的边。如果第二近的点所有的边都被搜索过了，那么检查第三靠近的点，以此类推。具体实现时需要维护一个尚未搜索过边的优先级队列。搜索一条边包括了首先计算查询点到有向边的终点的距离，然后根据距离长短替换优先级队列中的边，短边优先。

这里需要权衡的一项参数是距离的计算次数，当计算次数达到一个上限$M$后，返回距离查询点最近的点。

\begin{algorithm}
	% \begin{algorithm}[H] % 强制定位
	\caption{快速搜索算法\cite{harwood2016fanng}}
	\label{algo:fanng3}
	\begin{algorithmic}[1] %每行显示行号
		\Require $P$邻近图的顶点集，$E$邻近图的边集，$Q$查询点，$v$初始查询下标，$M$最大计算次数 % 输入
		\Ensure $n$最邻近下标 % 输出
		\State $X\leftarrow$ empty priority queue
		\State add edge $e_0$ with start vertex $P_v$ to $X$
		\State $m\leftarrow 1$
		\State $n\leftarrow v$
		\While {$m<M$}
		\State $e_i\leftarrow$ remove top of $X$
		\State $u\leftarrow$ index of end vertex of $e_i$
		\If {$P_u$ has not been searched yet}
		\State add edge $e_0$ with start vertex $P_u$ to $X$
		\State $m\leftarrow m+1$
		\If {$d(Q,P_u)<d(Q,P_n)$}
		\State $n\leftarrow u$
		\EndIf
		\EndIf
		\State $v\leftarrow$ index of start vertex of $e_i$
		\If {$i<$ number of edges with start vertex $P_v$}
		\State add edge $e_{i+1}$ with start vertex $P_v$ to $X$
		\EndIf
		\EndWhile
		\Return $n$
	\end{algorithmic}
\end{algorithm}

\subsection{邻近图的快速构建}

由于对$n$个点中的每个点产生的$n$种距离列表进行了排序，算法\ref{algo:fanng2}构建图的时间复杂度为$O(n^2\log{n})$，这样的时间复杂度使得建立邻近图的时间代价非常高，因此我们需要一种更有效的思路来构建邻近图。

这种思路使用了同时使用了两种不同的方法。其中第一种方法首先从数据集中随机选取了两个点$v_1$和$v_2$，然后使用算法\ref{algo:fanng1}尝试从$v_1$出发搜索$v_2$。如果目前的图为空图，这种方法会直接在$v_1$和$v_2$之间添加一条边。如果不是空图，则需要检验新加入的边是否造成了边的阻塞，如果有阻塞发生，则将较长的边移除以消除阻塞。如果数据集的大小为$N$,重复这种操作直至$50N$次，直至调用算法\ref{algo:fanng1}的平均成功率超过$90\%$为止。

当算法\ref{algo:fanng1}被不断调用时，图的构建过程会渐渐减慢，此时使用第二种方法来进一步提升图的完整性。第二种方法调用搜索算法返回几千个点的近邻点以构建近邻列表。遍历这个近邻列表并调用算法\ref{algo:fanng2}去除阻塞边。并行对数据集中的每个点都应用这种方法可以快速高效的构建邻近图。

\subsection{复杂度}

基于邻近图的近似搜索算法的时间复杂度约为$O(N^{0.2})$，其中$N$为数据集中的数据量\cite{harwood2016fanng}。因此，这种方法符合我们的系统准确、快速、高效、高维的要求。


